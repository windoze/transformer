Build the program
-----------------

```
./gradlew shadowJar
```

Run the program
---------------

```
java -jar build/libs/app.jar -p test/test.conf -l test/lookup-source.json
```

Test script:
------------

```shell
curl -v -XPOST -H"content-type:application/json" localhost:8000/process -d'{
    "requests": [
        {
            "pipeline": "t1",
            "data": {
                "fv": 97,
                "os": "Windows",
                "appBundle": "foo.bar.someApp",
                "osVersion": "11.0.1"
            }
        }
    ]
}'
```

```shell
curl -v -XPOST -H"content-type:application/json" localhost:8000/process -d'{
    "requests": [
        {
            "pipeline": "t1",
            "validate": true,
            "data": {
                "fv": 15,
                "os": "Windows",
                "appBundle": "foo.bar.someApp",
                "osVersion": "11.0.1"
            }   
        },
        {
            "pipeline": "t3",
            "validate": true,
            "data": {
                "ip": ["1.1.1.1", "24.48.0.1", "8.8.8.8"]
            }   
        }
    ]
}'

```

TODO:
-----
* CosmosDbSource
* AeroSpikeSource
* HttpJsonApiSource auth support
* Better Redis client that supports async batch

Differences with SparkSQL
-------------------------

There are numbers of differences between DSL and SparkSQL, most of them have alternatives, but some don't.
You must avoid the unsupported functions in Feathr feature definitions, otherwise these features cannot
be generated by this program.

* All reserved words must be in the lower cases, upper case reserved words are not recognized.
* String literal must by surrounded by `"`, `'` is not supported.
* Single line comments start with `#` instead of `--`, multi-line comments are not supported.
* `|` and `||` are not supported as they're conflict with DSL syntax, use `bit_or()` function and `or` operator instead.
* `=` is not supported, use `==` instead.
* `binary`, `decimal`, and `tinyint` data types are not supported.
* There is no separated `date` type, use `datetime` instead.
* The time zone is always set to UTC.
* All aggregation functions are not supported.
* All higher-order functions, that is the function takes another function as the parameter, are not supported.
* `cast(arg as TYPE)` function is not supported, use `boolean`, `int`, `bigint`, `float`, `double`, `string` functions instead,
  besides, `datetime` can be auto-converted to `string` and vice versa, the behavior is similar to most SparkSQL functions.
* Spark specific functions are not supported, including:
  * `current_catalog`
  * `current_database`
  * `current_user`
  * `spark_partition_id`
  * `monotonically_increasing_id`
* When exception happens, the pipeline is **not** stopped immediately, it will continue to run following transformations,
  but the error result of the expression will propagate to every following expression takes it as the input and its result
  will also be the same error, this process will move on until the pipeline ends, or the error is discarded explicitly, 
  e.g. by `ignore-error` transformation, or `where` transformation when the error is used in the condition expression.
* 
* 